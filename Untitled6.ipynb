{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f757a2e-9489-4e07-bd7a-28dde55d187f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Neutral' 'Positive']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34353ab64de94e43a24b5f708616df99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed81ba422d849bfb3ed01d68062fe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hannan\\AppData\\Local\\Temp\\ipykernel_8364\\32574848.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7800' max='7800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7800/7800 24:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.789400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.328200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./finbert-finetuned\\\\tokenizer_config.json',\n",
       " './finbert-finetuned\\\\special_tokens_map.json',\n",
       " './finbert-finetuned\\\\vocab.txt',\n",
       " './finbert-finetuned\\\\added_tokens.json',\n",
       " './finbert-finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Hannan\\Downloads\\stock_news.csv\")\n",
    "print(df['label'].unique())\n",
    "# Example: mapping string labels to integers\n",
    "label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# Keep only headline and label columns\n",
    "df = df[['headline', 'label']].dropna()\n",
    "\n",
    "# Split into train/test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example['headline'], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", num_labels=3)\n",
    "\n",
    "\n",
    "# Define compute metrics\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1_score(p.label_ids, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finbert-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./finbert-finetuned\")\n",
    "tokenizer.save_pretrained(\"./finbert-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feff61ed-5649-4997-9415-ad2aeb143cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9957154393196106}]\n",
      "                                       headline sentiment  confidence\n",
      "0              Nvidia stock surges to new highs   neutral    0.995116\n",
      "1  Oil prices drop sharply amid global tensions  positive    0.994545\n",
      "2    Investors cautious as recession fears grow  positive    0.992186\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pipeline using your saved model and tokenizer\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"./finbert-finetuned\",\n",
    "    tokenizer=\"./finbert-finetuned\"\n",
    ")\n",
    "result = sentiment_analyzer(\"Stock market is crashing due to inflation.\")\n",
    "print(result)\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataframe\n",
    "df = pd.DataFrame({\n",
    "    'headline': [\n",
    "        \"Nvidia stock surges to new highs\",\n",
    "        \"Oil prices drop sharply amid global tensions\",\n",
    "        \"Investors cautious as recession fears grow\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Apply sentiment model to each row\n",
    "def analyze(text):\n",
    "    result = sentiment_analyzer(text)[0]\n",
    "    return pd.Series([result['label'], result['score']])\n",
    "\n",
    "df[['sentiment', 'confidence']] = df['headline'].apply(analyze)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfa78e-0668-4021-9ca7-2191622178e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
